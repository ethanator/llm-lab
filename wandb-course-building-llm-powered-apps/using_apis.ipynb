{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LLM APIs\n",
    "\n",
    "We will explore OpenAI models API to generate text.\n",
    "\n",
    "<!--- @wandbcode{llmapps-intro} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "<details>\n",
    "    <summary>What does -qq mean in pip install?</summary>\n",
    "\n",
    "The `-qq` flag in pip install is used to minimize the output from the installation process. When you use `pip install` to install a Python package, it normally outputs a lot of information to the console, such as the names of the packages it's installing, their versions, and so on. If you don't want to see all this output, you can use the `-qq` option.\n",
    "\n",
    "Here's what each `q` means:\n",
    "\n",
    "- `-q`: Means \"quiet\". Using `-q` will provide less console output than the default. Warnings and errors will still be shown.\n",
    "- `-qq`: Means \"quieter\". Using `-qq` will provide even less console output. Only errors will be shown. \n",
    "\n",
    "So if you want to minimize the output from pip as much as possible, you can use `pip install -qq`. This can be useful in scripts and other automated contexts where you don't want a lot of console output.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>Give me a brief summary of the tiktoken Python package.</summary>\n",
    "    <a href=\"https://anaconda.org/conda-forge/tiktoken\">The tiktoken Python package</a> is a fast Byte Pair Encoding (BPE) tokenizer that is designed for use with OpenAI's models. BPE is a form of subword tokenization that is commonly used in natural language processing. The tiktoken package allows you to tokenize text in a way that is compatible with OpenAI's models, which can be useful when preparing text data for these models.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>Tell me more about Byte Pair Encoding.</summary>\n",
    "\n",
    "Byte Pair Encoding (BPE) is a type of subword tokenization method that is often used in natural language processing (NLP). It's a way of breaking down words into smaller units, which can help models handle words that aren't in their training data, among other benefits.\n",
    "\n",
    "Here's a high-level overview of how BPE works:\n",
    "\n",
    "Initialization: Start with a symbol vocabulary that contains each character in the alphabet (or byte pair if working with bytes) as a separate symbol. This vocabulary will be grown to include common combinations of symbols (which can be multi-character strings).\n",
    "\n",
    "Pair Statistics Calculation: On your training corpus, calculate the statistics of symbol pairs (how frequently each pair of symbols appears together).\n",
    "\n",
    "New Symbol Addition: Find the most frequently occurring pair of symbols, and add that pair as a new symbol to your vocabulary.\n",
    "\n",
    "Iteration: Repeat the pair statistics calculation and new symbol addition steps until you've reached a predefined vocabulary size or until a certain number of iterations have passed.\n",
    "\n",
    "The result of this process is that common character sequences (which often correspond to whole words or common parts of words) end up as single symbols in your vocabulary. This allows the model to handle a wide variety of words, including words that weren't in its training data (since it can break those words down into known subwords). It also gives the model a way to handle languages with large vocabularies or many compound words, like German.\n",
    "\n",
    "BPE has been used in several state-of-the-art models in NLP, such as GPT-2 and GPT-3 from OpenAI, and BERT from Google.\n",
    "\n",
    "For an example, consider the word 'lowly'. If 'low' and 'ly' are common tokens in the training corpus, BPE might treat 'low' and 'ly' as individual tokens, and 'lowly' would be tokenized as ['low', 'ly'].\n",
    "\n",
    "Please note that when using BPE, it's important to apply the same tokenization process to your input data when making predictions with the model, otherwise, the model might not be able to correctly interpret the input.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from pprint import pprint\n",
    "from wandb.integration.openai import autolog\n",
    "\n",
    "_ = load_dotenv()\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"using_apis.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need an OpenAI API key to run this notebook. You can get one [here](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    if any([\"VSCODE\" in x for x in os.environ.keys()]):\n",
    "        print(\"Please enter password in the VS Code prompt at the top of your VS Code window!\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enable W&B autologging to track our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33methan-ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ethan.chen/Desktop/Labs/llm-lab/wandb-course-building-llm-powered-apps/wandb/run-20230625_184226-nklpvfh3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ethan-ai/llmapps/runs/nklpvfh3' target=\"_blank\">fresh-sun-9</a></strong> to <a href='https://wandb.ai/ethan-ai/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ethan-ai/llmapps' target=\"_blank\">https://wandb.ai/ethan-ai/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ethan-ai/llmapps/runs/nklpvfh3' target=\"_blank\">https://wandb.ai/ethan-ai/llmapps/runs/nklpvfh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start logging to W&B\n",
    "autolog({\"project\":\"llmapps\", \"job_type\": \"introduction\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1135, 2337, 1222, 8436, 1386, 318, 7427, 0]\n",
      "Weights & Biases is awesome!\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "enc = encoding.encode(\"Weights & Biases is awesome!\")\n",
    "print(enc)\n",
    "print(encoding.decode(enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can decode the tokens one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135\tWe\n",
      "2337\tights\n",
      "1222\t &\n",
      "8436\t Bi\n",
      "1386\tases\n",
      "318\t is\n",
      "7427\t awesome\n",
      "0\t!\n"
     ]
    }
   ],
   "source": [
    "for token_id in enc:\n",
    "    print(f\"{token_id}\\t{encoding.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how the leading tokens contain spacing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample some text from the model. For this, let's create a wrapper function around the temperature parameters.\n",
    "Higher temperature will result in more random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_with_temperature(temp):\n",
    "    \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=\"Say something about Weights & Biases\",\n",
    "        max_tokens=50,\n",
    "        temperature=temp,\n",
    "    )\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEMP: 0, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is an amazing tool for tracking and analyzing machine '\n",
      " 'learning experiments. It provides powerful visualizations and insights into '\n",
      " 'model performance, enabling data scientists to quickly identify areas of '\n",
      " 'improvement and optimize their models.')\n",
      "('TEMP: 0.5, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is a powerful tool for tracking, analyzing, and visualizing '\n",
      " 'machine learning experiments. It provides an easy-to-use dashboard for '\n",
      " 'monitoring and comparing model performance, and it also offers a suite of '\n",
      " 'features to help you')\n",
      "('TEMP: 1, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is a powerful tool that helps data scientists and machine '\n",
      " 'learning engineers track, compare, and analyze machine learning experiments. '\n",
      " 'It allows users to visualize different aspects of model exploration, compare '\n",
      " 'model performance across experiments, and track metrics to')\n",
      "('TEMP: 1.5, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is an amazing tracking and visualization tool that allows '\n",
      " 'teams to extend the work accessible to Machine Learning teams while making '\n",
      " 'Machine Learning development and optimization iterations more transparent. '\n",
      " 'The web and mobile dashboards provide advanced analytics on model research '\n",
      " 'as')\n",
      "('TEMP: 2, GENERATION:  products\\n'\n",
      " '\\n'\n",
      " \"Weights & Biases' platform enables users to manage Experiment -> Training -> \"\n",
      " 'Govern & future their Production Machine Easier AI evaluation Driving Teams '\n",
      " 'faster time generate decision creates larger model ultimately achieve making '\n",
      " 'scalability process do rolling checklist necessary very informative user')\n"
     ]
    }
   ],
   "source": [
    "for temp in [0, 0.5, 1, 1.5, 2]:\n",
    "    pprint(f\"TEMP: {temp}, GENERATION: {generate_with_temperature(temp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the [`top_p` parameter](https://platform.openai.com/docs/api-reference/completions/create#completions/create-top_p) to control the diversity of the generated text. This parameter controls the cumulative probability of the next token. For example, if `top_p=0.9`, the model will pick the next token from the top 90% most likely tokens. The higher the `top_p` the more likely the model will pick a token that it hasn't seen before. You should only use one of `temperature` or `top_p` at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_topp(topp):\n",
    "    \"Generate text with a given top-p, higher top-p means more randomness\"\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=\"Say something about Weights & Biases\",\n",
    "        max_tokens=50,\n",
    "        top_p=topp,\n",
    "    )\n",
    "    return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TOP_P: 0.01, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is an amazing tool for tracking and analyzing machine '\n",
      " 'learning experiments. It provides powerful visualizations and insights into '\n",
      " 'model performance, enabling data scientists to quickly identify areas of '\n",
      " 'improvement and optimize their models.')\n",
      "('TOP_P: 0.1, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is an amazing tool for tracking and analyzing machine '\n",
      " 'learning experiments. It provides powerful visualizations and insights into '\n",
      " 'model performance, enabling data scientists to quickly identify areas of '\n",
      " 'improvement and optimize their models.')\n",
      "('TOP_P: 0.5, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is an amazing tool for tracking and analyzing machine '\n",
      " 'learning experiments. It provides powerful visualizations and metrics to '\n",
      " 'help teams better understand their models and make data-driven decisions. It '\n",
      " 'also offers collaboration features to help teams stay organized')\n",
      "('TOP_P: 1, GENERATION: \\n'\n",
      " '\\n'\n",
      " 'Weights & Biases is an AI platform designed to help data scientists better '\n",
      " 'analyze, visualize, and collaborate on their data. It provides advanced '\n",
      " 'tools to allow for real-time monitoring of machine learning experiments, '\n",
      " 'helping to quickly understand performance metrics,')\n"
     ]
    }
   ],
   "source": [
    "for topp in [0.01, 0.1, 0.5, 1]:\n",
    "    pprint(f'TOP_P: {topp}, GENERATION: {generate_with_topp(topp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to chat mode and see how the model responds to our messages. We have some control over the model's response by passing a `system-role`, here we can steer to model to adhere to a certain behaviour.\n",
    "\n",
    "> We are using `gpt-3.5-turbo`, this model is faster and cheaper than `davinci-003`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7VSviwcnw2XyhNEdpZmR6ARDyCIhk at 0x114b2e220> JSON: {\n",
       "  \"id\": \"chatcmpl-7VSviwcnw2XyhNEdpZmR6ARDyCIhk\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1687733730,\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Weights & Biases is a machine learning platform that helps data scientists and machine learning engineers track and visualize their experiments. It provides tools for experiment management, hyperparameter tuning, and model visualization, making it easier to understand and improve machine learning models. Weights & Biases also offers integrations with popular machine learning frameworks like TensorFlow, PyTorch, and Keras.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 27,\n",
       "    \"completion_tokens\": 74,\n",
       "    \"total_tokens\": 101\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Say something about Weights & Biases\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the response is a JSON object with relevant information about the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Weights & Biases is a machine learning platform that helps data scientists '\n",
      " 'and machine learning engineers track and visualize their experiments. It '\n",
      " 'provides tools for experiment management, hyperparameter tuning, and model '\n",
      " 'visualization, making it easier to understand and improve machine learning '\n",
      " 'models. Weights & Biases also offers integrations with popular machine '\n",
      " 'learning frameworks like TensorFlow, PyTorch, and Keras.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>usage/completion_tokens</td><td>▁▂▂▂▂▁▁▂▂█</td></tr><tr><td>usage/elapsed_time</td><td>▂▃▂▂▂▂▁█▃▅</td></tr><tr><td>usage/prompt_tokens</td><td>▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>usage/total_tokens</td><td>▁▂▂▂▂▁▁▂▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>usage/completion_tokens</td><td>74</td></tr><tr><td>usage/elapsed_time</td><td>3.17477</td></tr><tr><td>usage/prompt_tokens</td><td>27</td></tr><tr><td>usage/total_tokens</td><td>101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fresh-sun-9</strong> at: <a href='https://wandb.ai/ethan-ai/llmapps/runs/nklpvfh3' target=\"_blank\">https://wandb.ai/ethan-ai/llmapps/runs/nklpvfh3</a><br/>Synced 6 W&B file(s), 10 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230625_184226-nklpvfh3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Source of this notebook](https://github.com/wandb/edu/blob/main/llm-apps-course/notebooks/01.%20Using_APIs.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "gpt4all",
   "language": "python",
   "name": "gpt4all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
